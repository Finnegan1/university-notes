---
tags:
  - Algorithmik
  - zusammenfassung
---

# Markov-Kette und Hidden Markov Modell (HMM)

## Markov-Kette

Eine Markov-Kette ist ein stochastischer Prozess, der durch ZustÃ¤nde (z) und Ãœbergangswahrscheinlichkeiten $p(z|z')$ beschrieben wird.

## Hidden-Markov-Modell (HMM)

Ein Hidden-Markov-Modell (HMM) ist eine Markov-Kette, die in jedem Zustand z ein Zeichen a mit einer Emissionswahrscheinlichkeit $p(a|z)$ ausgibt.

**Charakterisiert durch:**
- **Startwahrscheinlichkeit** $p(z)$ fÃ¼r jeden Zustand z
- **Ãœbergangswahrscheinlichkeit** $p(z|z')$ zwischen zwei ZustÃ¤nden (von $z' \rightarrow z$)
- **Emissionswahrscheinlichkeit** $p(a|z)$ fÃ¼r die Ausgabe a im Zustand z

> [!example]- Beispiele fÃ¼r HMMs
> - **DNA-Sequenzen:** Die ZustÃ¤nde sind die Nukleotide A, C, G, T
> - **Gesprochene Sprache:** Die ZustÃ¤nde sind die Phoneme
> - **NachrichtenÃ¼bertragung durch ein Kanal:** Die verborgenen ZustÃ¤nde sind die gesendeten Zeichen, die empfangenen Zeichen die Ausgabe des HMM

# Viterbi-Algorithmus

## Ziel des Viterbi-Algorithmus:
- Rekonstruktion der wahrscheinlichsten Zufallsfolge fÃ¼r einen HMM aus seiner Ausgabe
- FÃ¼r eine Folge $a_{1}, \dots, a_{n}$ von Augabezeichen suchen wir eine Folge von ZustÃ¤nden $z_{1}, \dots, z_{n}$ , sodass $p(z_{1}, \dots, z_{n} | a_{1}, \dots, a_{n})$ maximal ist.

> [!note]- ðŸ“– ErklÃ¤rung der Notation $p(z_{1}, \dots, z_{n} | a_{1}, \dots, a_{n})$
> **Wie liest man das?**
>
> WÃ¶rtlich: "Die Wahrscheinlichkeit fÃ¼r die Zustandsfolge $z_{1}, \dots, z_{n}$, **gegeben** die Ausgabefolge $a_{1}, \dots, a_{n}$"
>
> **Was bedeutet der Strich "|"?**
>
> Der Strich "|" bedeutet **"gegeben"** oder **"unter der Bedingung"**.
>
> Die Formel bedeutet:
> - **Links vom Strich:** Was wir herausfinden wollen (die versteckten ZustÃ¤nde)
> - **Rechts vom Strich:** Was wir bereits wissen (die beobachteten Ausgaben)
>
> **Konkret fÃ¼r den Viterbi-Algorithmus:**
>
> Situation:
> - Du **beobachtest** eine Folge von Zeichen: $a_1, a_2, a_3, \ldots, a_n$
> - Du **kennst NICHT** die ZustÃ¤nde, die diese Zeichen produziert haben
> - Du **willst herausfinden:** Welche Zustandsfolge $z_1, z_2, z_3, \ldots, z_n$ hat am wahrscheinlichsten zu diesen Beobachtungen gefÃ¼hrt?
>
> Die Frage in der Wahrscheinlichkeitsnotation:
> $$p(z_1, \ldots, z_n | a_1, \ldots, a_n)$$
>
> bedeutet: "Wie wahrscheinlich ist die Zustandsfolge $z_1, \ldots, z_n$, **wenn wir bereits wissen**, dass die Ausgaben $a_1, \ldots, a_n$ sind?"

## Anwendung:
- NachrichtenÃ¼bertragung durch ein Kabel
- codierende Regionen der DNA
- Spracherkennung (Mustererkennung der Frequenz => Phoneme Ã¼ber HMM ermitteln)

## Vorgehen und mathematischer Hintergrund

Das Kernprinzip basiert auf dem Satz von Bayes, der die bedingte Wahrscheinlichkeit beschreibt:

$$p(z_1, \ldots, z_n | a_1, \ldots, a_n) = \frac{p(z_1, \ldots, z_n, a_1, \ldots, a_n)}{p(a_1, \ldots, a_n)}$$

Um das Vorgehen des Algorithmus zu verstehen, zerlegen wir diese Formel in ihre Komponenten.

> [!NOTE]- ZÃ¤hler: Die gemeinsame Wahrscheinlichkeit $p(Z, A)$
> Der ZÃ¤hler $p(z_1, \ldots, z_n, a_1, \ldots, a_n)$ ist die Wahrscheinlichkeit, dass eine *bestimmte Zustandssequenz* $Z$ und eine *bestimmte Beobachtungssequenz* $A$ gemeinsam auftreten. Man berechnet sie, indem man die Wahrscheinlichkeiten entlang eines einzigen Pfades multipliziert.
> 
> $$p(Z, A) = \underbrace{p(z_1)}_{\text{Start}} \cdot \underbrace{p(a_1|z_1)}_{\text{Emission}} \cdot \underbrace{p(z_2|z_1)}_{\text{Ãœbergang}} \cdot \underbrace{p(a_2|z_2)}_{\text{Emission}} \cdot \ldots$$
>
> Allgemein als Produktformel:
> $$p(Z, A) = p(z_1) \cdot \left( \prod_{i=2}^{n} p(z_i | z_{i-1}) \right) \cdot \left( \prod_{i=1}^{n} p(a_i | z_i) \right)$$
> 
> Dies ist der Wert, den der Viterbi-Algorithmus zu maximieren versucht.

> [!NOTE]- Nenner: Die Beobachtungswahrscheinlichkeit (Evidenz) $p(A)$
> Der Nenner $p(a_1, \ldots, a_n)$ ist die Gesamtwahrscheinlichkeit der Beobachtungssequenz $A$, summiert Ã¼ber *alle mÃ¶glichen* versteckten Zustandssequenzen $Z$:
> 
> $$p(A) = \sum_{\text{alle mÃ¶glichen } Z} p(Z, A)$$
> 
> **Problem:** Die direkte Berechnung ist extrem aufwÃ¤ndig, da die Anzahl der Pfade exponentiell wÃ¤chst ($K^n$ bei $K$ ZustÃ¤nden).
> **LÃ¶sung:** In der Praxis wird hierfÃ¼r der **Forward-Algorithmus** verwendet, der $p(A)$ effizient mittels dynamischer Programmierung berechnet.

> [!IMPORTANT] Verbindung zum Viterbi-Algorithmus: Maximierung statt Berechnung
> Der Viterbi-Algorithmus berechnet nicht die exakte Wahrscheinlichkeit $p(Z|A)$ fÃ¼r alle Pfade. Sein Ziel ist es, **den wahrscheinlichsten Pfad** $Z^*$ zu finden.
>
> $$ Z^* = \underset{Z}{\operatorname{argmax}} \, p(Z | A) = \underset{Z}{\operatorname{argmax}} \, \frac{p(Z, A)}{p(A)} $$
>
> **Der Trick:** Da der Nenner $p(A)$ fÃ¼r eine gegebene Beobachtung konstant ist und nicht von der Zustandsfolge $Z$ abhÃ¤ngt, beeinflusst er nicht, *welcher* Pfad der wahrscheinlichste ist. Wir kÃ¶nnen ihn bei der Maximierung also ignorieren.
>
> Das Problem vereinfacht sich zu:
> $$ Z^* = \underset{Z}{\operatorname{argmax}} \, p(Z, A) $$
>
> Der Viterbi-Algorithmus ist somit ein effizientes Verfahren (basierend auf dynamischer Programmierung), um den Pfad $Z^*$ zu finden, der die **gemeinsame Wahrscheinlichkeit** (den ZÃ¤hler) maximiert. Ein naiver Ansatz, alle Pfade zu prÃ¼fen, hÃ¤tte eine exponentielle Laufzeit und wÃ¤re unpraktikabel.

## Dynamic Programming LÃ¶sung

### Problemstellung

Alle mÃ¶glichen Zustandsfolgen durchzuprobieren hÃ¤tte **exponentielle Laufzeit** ($|Z|^n$ mÃ¶gliche Pfade). Die LÃ¶sung: **Dynamic Programming**

### Definition der DP-ZustandsgrÃ¶ÃŸe

Wir definieren:

$$t(z,k) := \max_{z_1,\dots,z_{k-1}} p(z_1,\dots,z_{k-1},z,a_1,\dots,a_k)$$

**Bedeutung:** Die maximale Wahrscheinlichkeit aller Pfade, die bei Zeit k im Zustand z enden und die Ausgaben $a_1,\dots,a_k$ erklÃ¤ren.

> [!tip] Kernidee
> Wir merken uns fÃ¼r jeden Zeitpunkt k und jeden Zustand z nur den **besten Weg dorthin**, weil ein nicht-optimaler Teilpfad niemals Teil eines optimalen Gesamtpfades sein kann (OptimalitÃ¤tsprinzip).

### Rekursionsformel (Bellman-Gleichung)

Betrachte den letzten Schritt eines optimalen Pfades, der in Zustand z zur Zeit k endet:
- Der Pfad muss aus irgendeinem Zustand $z'$ zur Zeit $k-1$ kommen
- Danach:
  - Ãœbergang $z' \to z$ mit Wahrscheinlichkeit $p(z|z')$
  - Emission $a_k$ aus Zustand z mit Wahrscheinlichkeit $p(a_k|z)$

Damit gilt:

$$
t(z,k) = \begin{cases}
p(z) \cdot p(a_1|z) & \text{fÃ¼r } k=1 \\
p(a_k|z) \cdot \max_{z'}\left[ t(z',k-1) \cdot p(z|z') \right] & \text{fÃ¼r } k>1
\end{cases}
$$

**Anfangsbedingung (k=1):** FÃ¼r das erste Symbol gibt es noch keinen VorgÃ¤nger, daher:
$$t(z,1) = p(z) \cdot p(a_1|z)$$

**Rekursionsschritt (k>1):** WÃ¤hle den VorgÃ¤ngerzustand $z'$, der das Maximum liefert:
$$t(z,k) = p(a_k|z) \cdot \max_{z'}\left[ t(z',k-1) \cdot p(z|z') \right]$$

> [!example]- Grafische Darstellung der Rekursion
> ```
>     t(z,k)
>        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
>        â”‚      â—‹      â”‚  â† Zustand z zur Zeit k
>        â”‚    p(a_k|z) â”‚
>        â”‚      â”‚      â”‚
>        â”‚   â”Œâ”€â”€â”´â”€â”€â”   â”‚
>     z'â‚â”‚  â—‹      â”‚   â”‚  â† mÃ¶gliche VorgÃ¤ngerzustÃ¤nde
>        â”‚  â”‚ p(z|z'â‚)â”‚
>     z'â‚‚â”‚  â—‹      â”‚   â”‚
>        â”‚  â”‚ p(z|z'â‚‚)â”‚
>        â””â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”˜
>          t(z',k-1)
> ```
> Wir wÃ¤hlen den VorgÃ¤nger $z'$, der $t(z',k-1) \cdot p(z|z')$ maximiert.

### Backtracking: Rekonstruktion der Zustandsfolge

Um den wahrscheinlichsten Pfad zu finden, gehen wir vom Ende ($t=n$) rÃ¼ckwÃ¤rts bis zum Start ($t=1$).

**Schritt 1: Besten Endzustand finden**
Wir wÃ¤hlen den Zustand mit der hÃ¶chsten Wahrscheinlichkeit in der letzten Spalte:
$$z_n^* = \underset{z}{\operatorname{argmax}} \, t(z,n)$$

**Schritt 2: RÃ¼ckwÃ¤rts iterieren**
FÃ¼r jeden Zeitschritt $k = n, \dots, 2$ bestimmen wir den VorgÃ¤nger $z_{k-1}^*$, der zum aktuellen Zustand $z_k^*$ gefÃ¼hrt hat.

Dazu gibt es zwei MÃ¶glichkeiten:

#### A) Mit gespeicherten Zeigern (Backpointer)
Falls wÃ¤hrend der Berechnung gespeichert wurde, welcher VorgÃ¤nger das Maximum geliefert hat, folgen wir einfach diesen Zeigern zurÃ¼ck.

#### B) Rechnerisch (ohne Zeiger)
Falls nur die Tabelle $t(z, k)$ vorliegt, berechnen wir den VorgÃ¤nger erneut, indem wir prÃ¼fen, welcher Ãœbergang das Maximum erzeugt:

$$z_{k-1}^* = \underset{z'}{\operatorname{argmax}} \left[ t(z', k-1) \cdot p(z_k^* | z') \right]$$

Hierbei ist:
- $z_k^*$: Der bereits ermittelte optimale Zustand zum Zeitpunkt $k$.
- $t(z', k-1)$: Der Tabellenwert des mÃ¶glichen VorgÃ¤ngers $z'$ zum Zeitpunkt $k-1$.
- $p(z_k^* | z')$: Die Ãœbergangswahrscheinlichkeit von $z'$ nach $z_k^*$.

> [!example] Beispiel zur rechnerischen Bestimmung
> Angenommen, der optimale Zustand bei $t=3$ ist **Regen** ($z_3^* = \text{Regen}$). Wir suchen den VorgÃ¤nger bei $t=2$.
>
> Wir berechnen fÃ¼r jeden mÃ¶glichen VorgÃ¤nger $z'$ den Wert:
> $$ \text{Score}(z') = \text{Tabellenwert}(z', 2) \cdot p(\text{Regen} | z') $$
>
> Der Zustand $z'$, der den hÃ¶chsten Score liefert, ist der gesuchte VorgÃ¤nger $z_{2}^*$.
> *Hinweis: Die Emissionswahrscheinlichkeit $p(a_k | z_k^*)$ wird hier nicht benÃ¶tigt, da sie ein konstanter Faktor ist und das Maximum bezÃ¼glich $z'$ nicht beeinflusst.*

### Laufzeitanalyse

- **ZustÃ¤nde:** $|Z|$
- **Zeitpunkte:** $n$
- **Pro DP-Zelle:** Maximum Ã¼ber $|Z|$ VorgÃ¤nger berechnen

**Gesamtlaufzeit:** $O(n \cdot |Z|^2)$

**Speicherplatz:** $O(n \cdot |Z|)$ fÃ¼r die DP-Tabelle

> [!success] Effizienzgewinn
> - **Naive LÃ¶sung:** $O(|Z|^n)$ â€“ exponentielle Laufzeit
> - **Viterbi-Algorithmus:** $O(n \cdot |Z|^2)$ â€“ polynomielle Laufzeit

### Numerisches Problem und LÃ¶sung

> [!warning] Problem: Numerischer Unterlauf
> Bei vielen Multiplikationen kleiner Wahrscheinlichkeiten (z.B. $0.01 \times 0.01 \times \ldots$) entstehen extrem kleine Zahlen, die zu Unterlauf fÃ¼hren kÃ¶nnen.

**LÃ¶sung: Rechnen im Log-Raum**

Statt $t(z,k)$ speichern wir $\log t(z,k)$:

$$
\log t(z,k) = \log p(a_k|z) + \max_{z'}\left[\log t(z',k-1) + \log p(z|z')\right]
$$

**Vorteile:**
- Multiplikationen â†’ Additionen âœ…
- Kein numerischer Unterlauf âœ…
- Das Maximum bleibt erhalten (Logarithmus ist monoton steigend)


[[Viterbi-Training]] 

